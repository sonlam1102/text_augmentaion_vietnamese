{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViHSD augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8l9Tql70sBA"
      },
      "source": [
        "pip install pyvi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA6c0ROaZTkG"
      },
      "source": [
        "# Filter data that need to augment \n",
        "\n",
        "# Filter label 1,2 - train on ViHSD dataset\n",
        "\n",
        "# Dataset analysis\n",
        "import pandas as pd \n",
        "\n",
        "DATA = 'drive/My Drive/CODE/HSD/dataset/train.csv'\n",
        "DATA_HATE = 'drive/My Drive/CODE/HSD/dataset/aug/data_label_1(6).csv'\n",
        "\n",
        "data = pd.read_csv(DATA, index_col=False)\n",
        "\n",
        "label1 = data.loc[data['label_id']==1]\n",
        "# label2 = data.loc[data['label_id']==2]\n",
        "\n",
        "# data_new = pd.concat([label1, label2])\n",
        "data_new = pd.concat([label1])\n",
        "\n",
        "print(data_new)\n",
        "data_new.to_csv(DATA_HATE, header=False, index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text augmentation using EDA techniques\n"
      ],
      "metadata": {
        "id": "Hkms9B0WtG1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change these arguments to fit with your own data / project\n",
        "\n",
        "class Argument:\n",
        "    input = \"drive/My Drive/CODE/HSD/dataset/aug/data_label_1(6).csv\"\n",
        "    output = \"drive/My Drive/CODE/HSD/dataset/aug/augmented_dataset(6).txt\"\n",
        "    num_aug = 8\n",
        "    alpha = 0.15\n",
        "\n",
        "\n",
        "args = Argument()"
      ],
      "metadata": {
        "id": "mDnd2VrXtMvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F46YUaLKZtCu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db52e0e3-3844-495b-811b-7cd6c18d5aad"
      },
      "source": [
        "# CODE augmetation: https://github.com/jasonwei20/eda_nlp\n",
        "\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "random.seed(1)\n",
        "import json\n",
        "\n",
        "\n",
        "# stop words list\n",
        "stop_words = []\n",
        "with open(\"drive/My Drive/CODE/HSD/vietnamese-stopwords.txt\", \"r\") as f:\n",
        "    stop_words = []\n",
        "    for line in f:\n",
        "        dd = line.strip('\\n')\n",
        "        stop_words.append(dd)\n",
        "\n",
        "# cleaning up text\n",
        "import re\n",
        "\n",
        "\n",
        "def get_only_chars(line):\n",
        "    # clean_line = \"\"\n",
        "\n",
        "    # line = line.replace(\"â€™\", \"\")\n",
        "    # line = line.replace(\"'\", \"\")\n",
        "    # line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    # line = line.replace(\"\\t\", \" \")\n",
        "    # line = line.replace(\"\\n\", \" \")\n",
        "    # line = line.lower()\n",
        "\n",
        "    # for char in line:\n",
        "    #     if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "    #         clean_line += char\n",
        "    #     else:\n",
        "    #         clean_line += ' '\n",
        "\n",
        "    # clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    # if clean_line[0] == ' ':\n",
        "    #     clean_line = clean_line[1:]\n",
        "    # return clean_line\n",
        "    return line\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Synonym replacement\n",
        "# Replace n words in the sentence with synonyms from wordnet\n",
        "########################################################################\n",
        "\n",
        "# for the first time you use wordnet\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            # print(\"replaced\", random_word, \"with\", synonym)\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:  # only replace up to n words\n",
        "            break\n",
        "\n",
        "    # this is stupid but we need it, trust me\n",
        "    sentence = ' '.join(new_words)\n",
        "    new_words = sentence.split(' ')\n",
        "\n",
        "    return new_words\n",
        "\n",
        "\n",
        "# def get_synonyms(word):\n",
        "# \tsynonyms = set()\n",
        "# \tfor syn in wordnet.synsets(word):\n",
        "# \t\tfor l in syn.lemmas():\n",
        "# \t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "# \t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "# \t\t\tsynonyms.add(synonym)\n",
        "# \tif word in synonyms:\n",
        "# \t\tsynonyms.remove(word)\n",
        "# \treturn list(synonyms)\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    with open(\"drive/My Drive/CODE/HSD/word_net_vi.json\", \"r\") as f:\n",
        "        wordnet = json.load(f)\n",
        "\n",
        "    for key, value in wordnet.items():\n",
        "        if key.strip() == word:\n",
        "            for v in value:\n",
        "                synonyms.add(v.strip())\n",
        "\n",
        "        if word in synonyms:\n",
        "            synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Random deletion\n",
        "# Randomly delete words from the sentence with probability p\n",
        "########################################################################\n",
        "\n",
        "def random_deletion(words, p):\n",
        "    # obviously, if there's only one word, don't delete it\n",
        "    if len(words) == 1:\n",
        "        return words\n",
        "\n",
        "    # randomly delete words with probability p\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        r = random.uniform(0, 1)\n",
        "        if r > p:\n",
        "            new_words.append(word)\n",
        "\n",
        "    # if you end up deleting all words, just return a random word\n",
        "    if len(new_words) == 0:\n",
        "        rand_int = random.randint(0, len(words) - 1)\n",
        "        return [words[rand_int]]\n",
        "\n",
        "    return new_words\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Random swap\n",
        "# Randomly swap two words in the sentence n times\n",
        "########################################################################\n",
        "\n",
        "def random_swap(words, n):\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        if len(new_words) > 0:\n",
        "            new_words = swap_word(new_words)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def swap_word(new_words):\n",
        "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
        "    random_idx_2 = random_idx_1\n",
        "    counter = 0\n",
        "    while random_idx_2 == random_idx_1:\n",
        "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
        "        counter += 1\n",
        "        if counter > 3:\n",
        "            return new_words\n",
        "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
        "    return new_words\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Random insertion\n",
        "# Randomly insert n words into the sentence\n",
        "########################################################################\n",
        "\n",
        "def random_insertion(words, n):\n",
        "    new_words = words.copy()\n",
        "    for _ in range(n):\n",
        "        add_word(new_words)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def add_word(new_words):\n",
        "    synonyms = []\n",
        "    counter = 0\n",
        "    while len(synonyms) < 1 and len(new_words) > 0:\n",
        "    # while len(synonyms) < 1:\n",
        "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        counter += 1\n",
        "        if counter >= 10:\n",
        "            return\n",
        "    \n",
        "    if len(new_words) > 0:\n",
        "        random_synonym = synonyms[0]\n",
        "        random_idx = random.randint(0, len(new_words) - 1)\n",
        "        new_words.insert(random_idx, random_synonym)\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# main data augmentation function\n",
        "########################################################################\n",
        "\n",
        "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
        "    sentence = get_only_chars(sentence)\n",
        "    words = sentence.split(' ')\n",
        "    words = [word for word in words if word is not '']\n",
        "    num_words = len(words)\n",
        "\n",
        "    augmented_sentences = []\n",
        "\n",
        "    if len(words) <= 0:\n",
        "        return augmented_sentences\n",
        "    num_new_per_technique = int(num_aug / 4) + 1\n",
        "    n_sr = max(1, int(alpha_sr * num_words))\n",
        "    n_ri = max(1, int(alpha_ri * num_words))\n",
        "    n_rs = max(1, int(alpha_rs * num_words))\n",
        "\n",
        "    # sr\n",
        "    for _ in range(num_new_per_technique):\n",
        "        a_words = synonym_replacement(words, n_sr)\n",
        "        augmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "    # ri\n",
        "    for _ in range(num_new_per_technique):\n",
        "        a_words = random_insertion(words, n_ri)\n",
        "        augmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "    # rs\n",
        "    for _ in range(num_new_per_technique):\n",
        "        a_words = random_swap(words, n_rs)\n",
        "        augmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "    # rd\n",
        "    for _ in range(num_new_per_technique):\n",
        "        a_words = random_deletion(words, p_rd)\n",
        "        augmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "    augmented_sentences = list(set(augmented_sentences))\n",
        "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
        "    shuffle(augmented_sentences)\n",
        "\n",
        "    # trim so that we have the desired number of augmented sentences\n",
        "    if num_aug >= 1:\n",
        "        augmented_sentences = augmented_sentences[:num_aug]\n",
        "    else:\n",
        "        keep_prob = num_aug / len(augmented_sentences)\n",
        "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
        "\n",
        "    # append the original sentence\n",
        "    augmented_sentences.append(sentence)\n",
        "\n",
        "    return augmented_sentences\n",
        "\n",
        "\n",
        "# the output file\n",
        "output = None\n",
        "if args.output:\n",
        "    output = args.output\n",
        "else:\n",
        "    from os.path import dirname, basename, join\n",
        "\n",
        "    output = join(dirname(args.input), 'eda_' + basename(args.input))\n",
        "\n",
        "# number of augmented sentences to generate per original sentence\n",
        "num_aug = 9  # default\n",
        "if args.num_aug:\n",
        "    num_aug = args.num_aug\n",
        "\n",
        "# how much to change each sentence\n",
        "alpha = 0.1  # default\n",
        "if args.alpha:\n",
        "    alpha = args.alpha\n",
        "\n",
        "\n",
        "# generate more data with standard augmentation\n",
        "def gen_eda(train_orig, output_file, alpha, num_aug=9):\n",
        "    try:\n",
        "        writer = open(output_file, 'w')\n",
        "        lines = open(train_orig, 'r').readlines()\n",
        "\n",
        "        writer.write(\"free_text\" + \",\" + \"label_id\" + '\\n')\n",
        "        augm = \"\"\n",
        "        for i, line in enumerate(lines):\n",
        "            try:\n",
        "                parts = line[:-1].split('|')\n",
        "                # print(parts)\n",
        "                # sen_id = parts[0]\n",
        "                label = parts[1]\n",
        "                sentence = parts[0]\n",
        "                aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
        "                for aug_sentence in aug_sentences:\n",
        "                    # writer.write(label + \"\\t\" + aug_sentence + '\\n')\n",
        "                    # writer.write(sen_id + \",\" +aug_sentence + \",\" + label + '\\n')\n",
        "                    augm = augm + aug_sentence + \",\" + label + '\\n'\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(parts)\n",
        "                pass\n",
        "\n",
        "        writer.write(augm)\n",
        "        writer.close()\n",
        "        print(\n",
        "            \"generated augmented sentences with eda for \" + train_orig + \" to \" + output_file + \" with num_aug=\" + str(\n",
        "                num_aug))\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated augmented sentences with eda for drive/My Drive/CODE/HSD/dataset/aug/data_label_1(6).csv to drive/My Drive/CODE/HSD/dataset/aug/augmented_dataset(6).txt with num_aug=8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main code. Run the cell below to generate new texts "
      ],
      "metadata": {
        "id": "kGRABedJtbSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main function. Run this cell to generate new data \n",
        "if __name__ == \"__main__\":\n",
        "    # generate augmented sentences and output into a new file\n",
        "    gen_eda(args.input, args.output, alpha=alpha, num_aug=num_aug)"
      ],
      "metadata": {
        "id": "ko36sAe1tAJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and concat with original "
      ],
      "metadata": {
        "id": "hGHUv4bftUlO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTAYSXXKu9vv"
      },
      "source": [
        "# TRAIN\n",
        "# concat train original with original\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DATA = 'drive/My Drive/CODE/HSD/dataset/train.csv'\n",
        "DATA_AUG = 'drive/My Drive/CODE/HSD/dataset/aug/augmented_dataset(3).txt'\n",
        "\n",
        "DATA_AUG_FINAL = 'drive/My Drive/CODE/HSD/dataset/aug/train_augmented_dataset(4).csv'\n",
        "\n",
        "data = pd.read_csv(DATA, index_col=False)\n",
        "data_hate = pd.read_csv(DATA_AUG, index_col=False, error_bad_lines=False)\n",
        "\n",
        "# data_hate = data_hate.iloc[: , 1:]\n",
        "data_hate.drop_duplicates(subset =\"free_text\", keep = False, inplace = True)\n",
        "\n",
        "data_aug = pd.concat([data, data_hate])\n",
        "\n",
        "data_aug.to_csv(DATA_AUG_FINAL, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibq7nv7mhWDG"
      },
      "source": [
        "# Extra augmentation\n",
        "import pandas as pd\n",
        "\n",
        "DATA = 'drive/My Drive/CODE/HSD/dataset/train.csv'\n",
        "\n",
        "DATA_AUG = 'drive/My Drive/CODE/HSD/dataset/aug/augmented_dataset(3).txt'\n",
        "DATA_AUG_2 = 'drive/My Drive/CODE/HSD/dataset/aug/augmented_dataset(6).txt'\n",
        "\n",
        "DATA_AUG_FINAL = 'drive/My Drive/CODE/HSD/dataset/aug/train_augmented_dataset(6).csv'\n",
        "\n",
        "data_hate = pd.read_csv(DATA_AUG, index_col=False, error_bad_lines=False)\n",
        "data_hate_2 = pd.read_csv(DATA_AUG_2, index_col=False, error_bad_lines=False)\n",
        "\n",
        "data_hate_final = pd.concat([data_hate, data_hate_2])\n",
        "data_hate_final.drop_duplicates(subset =\"free_text\", keep = False, inplace = True)\n",
        "\n",
        "data = pd.read_csv(DATA, index_col=False)\n",
        "data_aug = pd.concat([data, data_hate_final])\n",
        "\n",
        "data_aug.to_csv(DATA_AUG_FINAL, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}